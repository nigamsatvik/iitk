{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Satvik Nigam - nlp_quiz2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WKIPkrmmUIuh"},"source":["##  Softmax\n","Write softmax to convert vectors into probability distribution <br>\n","1) All the elements of probability distribution should be positive. <br>\n","2) Sum of all the elements of distribution will be 1. <br>\n","\n","$Softmax(x_i) = \n","\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"btXWSnSzUSwL","nbgrader":{"checksum":"28a5e10245b99b8ab2b48da1294931ce","grade":false,"grade_id":"cell-4294a66554d00ee0","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["import math\n","import numpy as np\n","def softmax(vector):\n","  \"\"\"\n","  Input : \n","      vector: np array of floats\n","  Output:\n","      distribution: np array of floats of same size as that of input converted by using softmax\n","  \"\"\"\n","  # YOUR CODE HERE\n","  distribution=np.array(vector)\n","  print(distribution)\n","  \n","  s=0\n","  for i in range(len(vector)):\n","    s+=math.exp(vector[i])\n","    distribution[i]=(math.exp(vector[i])/s)\n","  print(distribution)\n","  return distribution  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"jgSQYlhoUYqm","nbgrader":{"checksum":"8ff6ce9c264971c4fcbae23ee8ee2801","grade":true,"grade_id":"cell-17756755bdef637a","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"8566c4d8-5fd2-4176-d000-cf5a406667e5","executionInfo":{"status":"ok","timestamp":1561177516250,"user_tz":-330,"elapsed":1210,"user":{"displayName":"Satvik Nigam","photoUrl":"","userId":"12061167163106007112"}}},"source":["\n","#vector=np.array([1.2,3.4,9,5.5])\n","#softmax(vector)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[1.2 3.4 9.  5.5]\n","[1.         0.90024951 0.9959092  0.02919582]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([1.        , 0.90024951, 0.9959092 , 0.02919582])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZxZ2s-atUbGD"},"source":["### Convert to One hot\n","Given a vector containing a probability distribution, convert it to one-hot vector with the max index having 1 and rest of the indices having 0"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"nXonICNLUtHI","nbgrader":{"checksum":"f5705ac653b727a1382d43c8b0316aad","grade":false,"grade_id":"cell-ba97ad83ad5b99e6","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["from keras.utils import to_categorical\n","def convert_to_one_hot(p):\n","  \n","  \"\"\"\n","  Inputs:\n","    p: numpy array of 1 dimension, probability distribution\n","  Outputs:\n","    oh: one_hot vector corresponding to p\n","  \"\"\"\n","  # YOUR CODE HERE\n","  \n"," \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"c2ZuIvAdVNCK","nbgrader":{"checksum":"92856f376c2458b0ef41c939ae27f397","grade":true,"grade_id":"cell-dae6f55da8097488","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4ZXt3L4zVk2l"},"source":["## Complete analogy \n","\n","**Cosine Similarity:**\n","We can find the similarity in terms of  angle between two vectors. Formally, the Cosine Similarity  is  between two vectors  p  and  q  is defined as:\n","\n","$s = \\frac{p⋅q}{||p||||q||} $, where s∈[−1,1]<br>\n","Implement the function"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"5o7TWJYDVk48","nbgrader":{"checksum":"057c75e1bb9747b6827147dd16add597","grade":false,"grade_id":"cell-c710883418fe64f9","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["import math\n","import numpy as np\n","def cosine_similarity(v1,v2):\n","    \"\"\"\n","    Input:\n","        v1: numpy array \n","        v2: numpy array\n","        \n","    Output:\n","        v: single floating point value\n","        \n","        v = cosine similarity between v1 and v2 = (v1 dot v2)/{||v1||*||v2||}\n","    \"\"\"\n","    # YOUR CODE HERE\n","    a=v1.dot(v2)\n","    b=np.linalg.norm(v1)*np.linalg.norm(v2)\n","    v=a/b\n","    return v"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"IBRXJTiVVk7F","nbgrader":{"checksum":"ca10ad33f541472c8e6e4ba85c99f401","grade":true,"grade_id":"cell-1ddefb3eb9ea742f","locked":true,"points":3,"schema_version":1,"solution":false},"colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ayLS2H-LVk89"},"source":["Consider the words $a, b, c, d$ and their corresponding word vectors $x_a, x_b, x_c, x_d$ such that they have this analogical relationship\n","$a:b :: c:d$ <br>\n","For eg.,<br>\n","Princess: Queen : : Prince : ? <br>\n","Complete the analogy by finding the missing word. <br>\n","To find out missing word \"d\", you need to find the word vector which has maximum cosine similarity with the vector $x_b - x_a + x_c$. The word corresponding to this vector is the word that will complete the analogy. In other words, you need to implment the following function <br>\n","$d = argmax_{x_i \\in \\mathcal{X}} \\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||\\cdot ||x_i||}$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"caAFxm09V2bE","nbgrader":{"checksum":"73bfd294eacef5166359054db16e971d","grade":false,"grade_id":"cell-9e2fdcaab9a983a6","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def complete_analogy(a, b, c, word_dict):\n","  \"\"\"\n","  Inputs:\n","    a, b, c: strings, with analogical relationship as described above\n","    word_dict: dictionary, dictionary with keys as words and values as corresponding word vectors\n","  Output:\n","      missing: str, the missing word d\n","  \"\"\"\n","  # YOUR CODE HERE\n","  for i in word_dict:\n","    if (i==a):\n","      xa=word_dict.get(i)\n","    else if (i==b):\n","      xb=word_dict.get(i)\n","    else if (i==c):\n","      xc=word_dict.get(i)\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"2kwj81EHV3pz","nbgrader":{"checksum":"76ebd769ce21a3f481e4d29a8bdd687e","grade":true,"grade_id":"cell-63c4d958a4192123","locked":true,"points":3,"schema_version":1,"solution":false},"colab":{}},"source":["word_dict = {'princess':\t[-1.720603,\t-3.560657],\n","             'queen':\t[-0.722603,\t-1.232549],\n","\t           'girl':\t[-2.789075,\t-3.869762],\n","             'king':\t[-0.370373,\t0.576843],\n","            'prince':\t[-1.693504,\t0.719822],\n","            'toy':\t[2.78,\t-0.71],\n","            'lady':\t[-1.693,\t-0.7192],\n","            'student':\t[-1.693504,\t0.719822]}\n","\n","'''test for complete_analogy'''\n","def test_complete_analogy():\n","test_complete_analogy()"],"execution_count":0,"outputs":[]}]}