{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Satvik Nigam - Backpropagation_XOR_f.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ly0FEPLRRA3R"},"source":["## Implementing Back-propagation Algorithm with XOR data\n","\n","### XOR data: <br>\n","**$ x_0 \\ x_1 \\ y$** <br>\n","$0 \\ \\ \\  0 \\ \\ \\  0$ <br>\n","$0 \\ \\ \\  1 \\ \\ \\  1$ <br>\n","$1 \\ \\ \\  0 \\ \\ \\  1$<br>\n","$1 \\ \\ \\  1 \\ \\ \\  0$<br>\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Iewd7ysumt1L","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xYnKrpHRC88X"},"source":["##Activation function\n","\n","Sigmoid function $$\\frac{1}{1+ e^{-x}} $$"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"RR5ReWLb0rQ8","nbgrader":{"checksum":"f4effe0bb558b3f87da12a7a5133ee75","grade":false,"grade_id":"cell-d84ad9dbcb889c3f","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["#Define our activation function\n","\n","def sigmoid (x):\n","    '''\n","    Input:\n","        x: numpy array of any shape\n","    Output:\n","        y: numpy array of same shape as x\n","    '''\n","    # YOUR CODE HERE\n","    y=(1/(1+np.exp(-x)))\n","    return y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"4q5l-n3LkamT","nbgrader":{"checksum":"8556b6d6d8fb50561b6a4b23e4f428e2","grade":true,"grade_id":"cell-80b53c7b5034f924","locked":true,"points":1,"schema_version":1,"solution":false},"outputId":"23508362-3b7a-4fbd-ed1b-f0844a8642ed","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1559991681810,"user_tz":-330,"elapsed":1373,"user":{"displayName":"Satvik Nigam","photoUrl":"","userId":"12061167163106007112"}}},"source":["'''Testing'''\n","assert sigmoid(0)==0.5\n","assert np.isclose(sigmoid(-2), 0.119202922, atol=0.0001)\n","print('Test passed', '\\U0001F44D')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Test passed üëç\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"hAS1d1_Wkama","nbgrader":{"checksum":"b5988d9e371e77c56c63d315f5c63d3e","grade":false,"grade_id":"cell-9ebb909521c85d2b","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["# Define the activation function derivative\n","\n","def sigmoid_derivative(x):\n","    '''\n","    Input:\n","        x: numpy array of any shape\n","    Output:\n","        y: numpy array of same shape as x\n","        y = derivative of sigmoid\n","    '''\n","    # YOUR CODE HERE\n","    #o=np.ones(x)\n","    y=(sigmoid(x))(1-sigmoid(x))\n","    return y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"D27idxL-kami","nbgrader":{"checksum":"d980605ecd57b9e69ac9a07ec6cfbc06","grade":true,"grade_id":"cell-8668ae928d66bf7c","locked":true,"points":1,"schema_version":1,"solution":false},"outputId":"31798bd3-3e9f-4991-85d1-92c38617b129","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"error","timestamp":1559995223072,"user_tz":-330,"elapsed":987,"user":{"displayName":"Satvik Nigam","photoUrl":"","userId":"12061167163106007112"}}},"source":["'''Testing code for sigmoid_derivative'''\n","assert sigmoid_derivative(1) == 0\n","assert sigmoid_derivative(0) == 0\n","print('Test passed', '\\U0001F44D')\n"],"execution_count":19,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-2f27cea5d3e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''Testing code for sigmoid_derivative'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test passed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\U0001F44D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-a3b0711e6e91>\u001b[0m in \u001b[0;36msigmoid_derivative\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#o=np.ones(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7QEVx1qpYo3m"},"source":["## Defining the model"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"QiXPUCkdP5ky","nbgrader":{"checksum":"4646c91497019ffdff3cd615c01b92a9","grade":false,"grade_id":"cell-4a7dcd60006d48dc","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["#Define the NeuralNetwork class\n","\n","class NeuralNetwork:\n","    def __init__(self, net_arch):\n","        '''   \n","        Input:\n","            net_arch: list of 3 integers\n","        Action:\n","            Creates instance variables:\n","                self.input: np array of shape (ni,1)\n","                self.layer1: nprarray of shape (nh,1)\n","                self.output: np array of shape (no,1)\n","                self.weights1: np array of shape (nh, ni), initialized randomly between (-1,1)\n","                self.weights2: np array of shape (no, nh), initialized randomly between (-1,1)\n","                \n","            NOTE: We do not use bias explicitly here. Input x can have the first element 1 to have a bias term.\n","        '''\n","        ni = net_arch[0]  ## Number of neurons in input layer    \n","        nh = net_arch[1]  ## Number of neurons in hidden layer\n","        no = net_arch[2]  ## Number of neurons in output layer\n","        \n","        self.ni = ni\n","        self.nh = nh\n","        self.no = no\n","        \n","        # YOUR CODE HERE\n","        \n","    def feedforward(self,x):\n","        '''\n","        Input:\n","            x: numpy array of shape (ni,1)\n","        Action:\n","            \n","        Return:\n","            output: numpy array of shape (no,1),\n","        '''\n","        # YOUR CODE HERE\n","             \n","\n","    def backprop(self,x,y,eta):\n","        '''\n","        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n","        Input:\n","            x: numpy array of shape (ni,1)\n","            y: numpy array of shape (no,1)\n","            eta: learning rate\n","        Action:\n","        # Finding the derivatives\n","            del_weights2: np array of shape (no,nh) that stores the derivative of the loss function with respect to weights2\n","            del_weights1: np array of shape (nh,ni) that stores the derivative of the loss function with respect to weights1\n","            \n","        # Update the weights with the derivative of the loss function\n","            weights1 += eta*del_weights1\n","            weights2 += eta*del_weights2\n","        '''\n","   \n","        # YOUR CODE HERE\n","\n","    def fit(self, X, Y, eta, epochs):\n","        '''\n","        input:\n","        X: training input data of shape (4,2)\n","        Y: training output of shape (4,1)\n","        eta: learning rate\n","        epochs: number of epochs\n","        Action:\n","        # Modify the input by adding ones of shape(4,1) \n","        # Set up the feed-forward propagation for the modified input   \n","        # Set up the back-propagation of the error to adjust the weights\n","        '''\n","        # YOUR CODE HERE\n","        \n","    def predict(self,x,y):\n","        '''\n","        # Predict function is used to check the prediction result of the neural network\n","        Input:\n","        x: single input data of shape (1,3)\n","        y: single output data of shape (1,1)\n","        Action\n","        pred_out: predict the output based on the model using feedforward\n","        \n","        Output\n","        error: y - pred_out\n","        \n","        \n","        '''\n","        # YOUR CODE HERE\n","        return(error)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"6gp5eg0skamy","nbgrader":{"checksum":"01c984c866bc53a22934b3a8a85bb6ae","grade":true,"grade_id":"cell-640694d6a41609e8","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{}},"source":["'''Testing code for __init__'''\n","\n","net_arch = [3,4,1]\n","nn1 = NeuralNetwork(net_arch)\n","assert nn1.input.shape==(3,1)\n","assert nn1.layer1.shape == (4,1)\n","assert nn1.output.shape == (1,1)\n","assert np.all(nn1.weights1 < 1)\n","print('Test passed', '\\U0001F44D')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"hP6uPl-2kanA","nbgrader":{"checksum":"2bcf8c9fbcf3690b31a8c6a9cde70928","grade":true,"grade_id":"cell-f0a271c06ea8c25b","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{}},"source":["'''Testing code for feedforward'''\n","\n","def feedforward_original(nn1,x):\n","    assert x.shape == (nn1.ni, 1)\n","    layer1 = sigmoid(np.dot(nn1.weights1, x))\n","    output = sigmoid(np.dot(nn1.weights2, layer1))\n","    return output\n","x = np.array([0,1,1]).reshape(-1, 1)\n","assert nn1.feedforward(x) == feedforward_original(nn1, x)\n","print('Test passed', '\\U0001F44D')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"XBXXqFAKYxnv","nbgrader":{"checksum":"3737666b17aaad1d4a58603c7b67ce81","grade":true,"grade_id":"cell-a4a2893378d844f3","locked":true,"points":4,"schema_version":1,"solution":false},"colab":{}},"source":["'''Testing code for backprop'''\n","def backprop_original(nn1,x,y,eta):\n","    weights1 = nn1.weights1\n","    weights2 = nn1.weights2\n","    del_weights2 = np.dot(((y - nn1.output) * sigmoid_derivative(nn1.output)),nn1.layer1.reshape(-1, 1).T)\n","    del_weights1 = np.dot(((y - nn1.output) * sigmoid_derivative(nn1.output)*nn1.weights2.T * sigmoid_derivative(nn1.layer1)), x.T)\n","\n","    # update the weights with the derivative (slope) of the loss function\n","    weights1 += eta*del_weights1\n","    weights2 += eta*del_weights2\n","    return(weights1, weights2)\n","\n","x = np.array([0,1,1]).reshape(-1, 1)\n","y = np.array([[0],])\n","eta = 1\n","nn1.backprop(x, y, eta)\n","w1, w2 = backprop_original(nn1, x, y, eta) \n","assert np.all(np.isclose(w1, nn1.weights1))\n","assert np.all(np.isclose(w2, nn1.weights2))\n","print('Test passed', '\\U0001F44D')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tUs7Wwu7ZjBX"},"source":["## Fitting the data (Training)"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"kbRcD6R6RBpw","nbgrader":{"checksum":"dbbf11073bfdce0010e10e0eaca6500c","grade":false,"grade_id":"cell-e59bb4a5a7ddab07","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["## CHECK THE PERFORMANCE\n","'''\n","Input:\n","# Set the input data\n","X = ([[0.1, 0.1], [0.1, 0.9],\n","                [0.9, 0.1], [0.9, 0.9]])\n","# Set the labels, the correct results for the xor operation\n","Y = ([[0.1], [0.9], \n","                 [0.9], [0.1]])\n","Action:\n","# Initialize the NeuralNetwork with\n","# 3 input neurons\n","# 4 hidden neurons\n","# 1 output neuron\n","\n","# Fit the datas\n","'''\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"-aYoNXSF2f9F","nbgrader":{"checksum":"b1a894b26291672207fabd649f07486d","grade":true,"grade_id":"cell-7391ad1a93e30118","locked":true,"points":3,"schema_version":1,"solution":false},"colab":{}},"source":["'''Testing for fit'''\n","X = np.array([[0.1, 0.1], [0.1, 0.9],\n","                [0.9, 0.1], [0.9, 0.9]])\n","# Set the labels, the correct results for the xor operation\n","Y = np.array([[0.1], [0.9], \n","                 [0.9], [0.1]])\n","nn1.fit(X,Y,1,10000)\n","x = np.array([1,1,1]).reshape(-1, 1)\n","y = np.array([[0],])\n","print(nn1.feedforward(x),y)\n","assert np.all(np.isclose(nn1.feedforward(x),y,atol=0.1))\n","print('Test passed', '\\U0001F44D')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SLXYzUB4fs8o"},"source":["## Plotting "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H2XKM2sufuwL","colab":{}},"source":["def plotting(X, Y):\n","  x_plot = X.T\n","  color = []\n","  for i in Y:\n","    if i[0] == 0:\n","      color.append('g')\n","    else:\n","      color.append('r')\n","  color = np.array(color)\n","  print(x_plot)\n","  plt.figure()\n","  plt.xlabel('x1')\n","  plt.ylabel('x2')\n","  plt.scatter(x_plot[0],x_plot[1],color=color)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PWvKv62qsiLP","colab":{}},"source":["plotting(X, Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K1PPPHD9aDM-"},"source":["## Could you test it now?\n","\n","Find the error between the predicted output and the desired output."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XGQZuMbihk-1","colab":{}},"source":["def testing(X, Y):\n","  ones = 0.9*np.ones((X.shape[0],1))\n","  x_test = np.concatenate([ones, X], axis=1)\n","  y_test = Y\n","\n","  for k in range(4):\n","    print(nn.predict(x_test[k].reshape(-1, 1),y_test[k]))\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"vMqNNPopYjPI","nbgrader":{"checksum":"4ff21be5bf4c5c721cbdbb0d612a9e0c","grade":true,"grade_id":"cell-08caa0ed7b0ce465","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{}},"source":["'''Testing the prediction'''\n","x = np.array([0.9,0.9,0.9]).reshape(-1, 1)\n","y = np.array([[0.1],])\n","\n","assert np.all(np.isclose(nn1.predict(x,y),0, atol=0.01))\n","print('Test passed', '\\U0001F44D')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9-0oU2fCdhRN"},"source":["# Advanced\n","## Does the performance increase with increasing the number of neurons in the hidden layer?\n","- Repeat the training with 1 neuron in the hidden layer, then with 3 neuron and then with 5 neuron in the hidden layer to see the trend in performance\n","- Compare the training error\n","- Compare the testing error"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SNHBGSLBD_t2","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}